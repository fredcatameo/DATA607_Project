{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Data Preparing Process\n",
    "1. [KeyBERT (all-MiniLM-L6-v2)](https://maartengr.github.io/KeyBERT/): 2-gram Keywords Extraction and Embedding\n",
    "\n",
    "2. [Kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html): Topic Clustering with Embedding Vector\n",
    "\n",
    "3. Reviews Selection: Select top 200 reviews from 10 topics.\n",
    "\n",
    "4. [Grok2-latest(LLM)](https://x.ai/news/grok-2):  Summary Soft-Labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "load_dotenv('.env')\n",
    "\n",
    "from keybert import KeyBERT\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24354, 23)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/silver_20250324_Airlines_Reviews_Sentiment.csv')\n",
    "reviews_data  = pd.DataFrame(data.Review)\n",
    "reviews_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [KeyBERT (all-MiniLM-L6-v2)](https://maartengr.github.io/KeyBERT/): 2-gram Keywords Extraction and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyBERT 2-gram keywords extraction and embedding\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "embedding_model = kw_model.model\n",
    "\n",
    "def extract_keywords_and_embeddings(text, top_n=8):\n",
    "    keywords_with_scores = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(2, 2),\n",
    "        stop_words='english',\n",
    "        use_mmr=True,\n",
    "        top_n=top_n\n",
    "    )\n",
    "    keywords = [kw for kw, _ in keywords_with_scores]\n",
    "    embeddings = embedding_model.embed(keywords)\n",
    "    return keywords, embeddings\n",
    "\n",
    "reviews_data[['keywords_2gram', 'keywords_2gram_emb']] = reviews_data['Review'].apply(\n",
    "    lambda x: pd.Series(extract_keywords_and_embeddings(str(x)))\n",
    ")\n",
    "\n",
    "#reviews_data.to_csv('Airlines_Reviews_2gramEmbedding.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [Kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html): Topic Clustering with Embedding Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Average Embedding as Reviews Representative Vector\n",
    "def average_embedding(emb_list):\n",
    "  return np.mean(emb_list, axis=0)\n",
    "\n",
    "reviews_data['review_vector'] = reviews_data['keywords_2gram_emb'].apply(average_embedding)\n",
    "reviews_data.dropna(inplace=True)\n",
    "\n",
    "review_vectors = np.vstack(reviews_data['review_vector'].values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clustering\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(review_vectors)\n",
    "\n",
    "reviews_data['cluster'] = cluster_labels\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "reviews_data['distance_to_center'] = [\n",
    "    np.linalg.norm(vec - cluster_centers[label])\n",
    "    for vec, label in zip(review_vectors, cluster_labels)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reviews Selection: Select top 200 reviews from 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP = 200\n",
    "top_per_cluster = (\n",
    "    reviews_data\n",
    "    .sort_values(by='distance_to_center')\n",
    "    .groupby('cluster')\n",
    "    .head(TOP)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "top_per_cluster[['cluster', 'Review', 'distance_to_center']].shape\n",
    "\n",
    "# top_per_cluster.to_csv('Reviews_For_Labeling.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. [Grok2-latest(LLM)](https://x.ai/news/grok-2):  Summary Soft-Labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>keywords_2gram</th>\n",
       "      <th>keywords_2gram_emb</th>\n",
       "      <th>review_vector</th>\n",
       "      <th>cluster</th>\n",
       "      <th>distance_to_center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7272</td>\n",
       "      <td>Solid airline, will fly again. Professional, ...</td>\n",
       "      <td>['flight delayed', 'boarding literally', 'rema...</td>\n",
       "      <td>[[ 6.78944439e-02 -4.25142385e-02 -2.41788756e...</td>\n",
       "      <td>[ 2.31793001e-02  2.13883575e-02 -1.60253830e-...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.244511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20018</td>\n",
       "      <td>Worst experience ever, the long delays, to r...</td>\n",
       "      <td>['flight cancelled', 'melbourne airport', 'cal...</td>\n",
       "      <td>[[ 0.04839371 -0.00492003  0.00085667 ... -0.0...</td>\n",
       "      <td>[ 1.53331067e-02  9.13699530e-03 -9.21477564e-...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.245582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15726</td>\n",
       "      <td>I flew with NIKI this week from Vienna to Al...</td>\n",
       "      <td>['flew niki', 'vienna alicante', 'expensive re...</td>\n",
       "      <td>[[-0.01471053  0.01716902 -0.02504953 ... -0.0...</td>\n",
       "      <td>[-2.02544983e-02  2.36782003e-02 -1.38547458e-...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.246489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16527</td>\n",
       "      <td>Osaka to Hong Kong. I have flown Peach severa...</td>\n",
       "      <td>['osaka hong', 'flown peach', 'cost airline', ...</td>\n",
       "      <td>[[ 0.02480116  0.00897194  0.02534627 ... -0.0...</td>\n",
       "      <td>[ 1.52944177e-02  1.75527278e-02 -8.45169462e-...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.247793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11589</td>\n",
       "      <td>I had to change my flight route to Venice a ...</td>\n",
       "      <td>['helpful flight', 'chisinau venice', 'drink s...</td>\n",
       "      <td>[[ 0.0440908   0.0415143  -0.00632484 ...  0.0...</td>\n",
       "      <td>[ 9.40260012e-04  9.34662111e-03 -1.31993005e-...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.249408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>7684</td>\n",
       "      <td>Johannesburg to Durban return on Comair (Bri...</td>\n",
       "      <td>['durban return', 'late flight', 'jnb bag', 'a...</td>\n",
       "      <td>[[-0.06542107  0.04507969 -0.0499328  ... -0.0...</td>\n",
       "      <td>[ 1.57586411e-02  3.99073912e-03 -3.65359411e-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>10887</td>\n",
       "      <td>CDG-TBS and TBS-AMS. Brezhnev era service and ...</td>\n",
       "      <td>['cdg tbs', 'georgia recommend', 'airline trav...</td>\n",
       "      <td>[[-0.04120119 -0.00643476  0.03505153 ... -0.1...</td>\n",
       "      <td>[-1.72636751e-02  2.09203716e-02 -1.16959680e-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>10886</td>\n",
       "      <td>CDG-TBS and TBS-AMS. Brezhnev era service and ...</td>\n",
       "      <td>['cdg tbs', 'georgia recommend', 'airline trav...</td>\n",
       "      <td>[[-0.04120119 -0.00643476  0.03505153 ... -0.1...</td>\n",
       "      <td>[-1.72636751e-02  2.09203716e-02 -1.16959680e-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>20932</td>\n",
       "      <td>Leon Bajio to Puerto Vallarta. Punctual airl...</td>\n",
       "      <td>['bajio puerto', 'punctual airline', 'flight c...</td>\n",
       "      <td>[[ 0.0021889   0.03311267 -0.04355304 ... -0.0...</td>\n",
       "      <td>[ 1.26226731e-02  4.72505391e-02 -1.89940874e-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>8857</td>\n",
       "      <td>We had a return flight to Ho Chi Minh from B...</td>\n",
       "      <td>['vietnam boarding', 'return trip', 'zurich ed...</td>\n",
       "      <td>[[ 0.02665213  0.01401893 -0.06211667 ...  0.0...</td>\n",
       "      <td>[-9.46927350e-03  2.49222778e-02  2.05155313e-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             Review  \\\n",
       "0           7272   Solid airline, will fly again. Professional, ...   \n",
       "1          20018    Worst experience ever, the long delays, to r...   \n",
       "2          15726    I flew with NIKI this week from Vienna to Al...   \n",
       "3          16527   Osaka to Hong Kong. I have flown Peach severa...   \n",
       "4          11589    I had to change my flight route to Venice a ...   \n",
       "...          ...                                                ...   \n",
       "1995        7684    Johannesburg to Durban return on Comair (Bri...   \n",
       "1996       10887  CDG-TBS and TBS-AMS. Brezhnev era service and ...   \n",
       "1997       10886  CDG-TBS and TBS-AMS. Brezhnev era service and ...   \n",
       "1998       20932    Leon Bajio to Puerto Vallarta. Punctual airl...   \n",
       "1999        8857    We had a return flight to Ho Chi Minh from B...   \n",
       "\n",
       "                                         keywords_2gram  \\\n",
       "0     ['flight delayed', 'boarding literally', 'rema...   \n",
       "1     ['flight cancelled', 'melbourne airport', 'cal...   \n",
       "2     ['flew niki', 'vienna alicante', 'expensive re...   \n",
       "3     ['osaka hong', 'flown peach', 'cost airline', ...   \n",
       "4     ['helpful flight', 'chisinau venice', 'drink s...   \n",
       "...                                                 ...   \n",
       "1995  ['durban return', 'late flight', 'jnb bag', 'a...   \n",
       "1996  ['cdg tbs', 'georgia recommend', 'airline trav...   \n",
       "1997  ['cdg tbs', 'georgia recommend', 'airline trav...   \n",
       "1998  ['bajio puerto', 'punctual airline', 'flight c...   \n",
       "1999  ['vietnam boarding', 'return trip', 'zurich ed...   \n",
       "\n",
       "                                     keywords_2gram_emb  \\\n",
       "0     [[ 6.78944439e-02 -4.25142385e-02 -2.41788756e...   \n",
       "1     [[ 0.04839371 -0.00492003  0.00085667 ... -0.0...   \n",
       "2     [[-0.01471053  0.01716902 -0.02504953 ... -0.0...   \n",
       "3     [[ 0.02480116  0.00897194  0.02534627 ... -0.0...   \n",
       "4     [[ 0.0440908   0.0415143  -0.00632484 ...  0.0...   \n",
       "...                                                 ...   \n",
       "1995  [[-0.06542107  0.04507969 -0.0499328  ... -0.0...   \n",
       "1996  [[-0.04120119 -0.00643476  0.03505153 ... -0.1...   \n",
       "1997  [[-0.04120119 -0.00643476  0.03505153 ... -0.1...   \n",
       "1998  [[ 0.0021889   0.03311267 -0.04355304 ... -0.0...   \n",
       "1999  [[ 0.02665213  0.01401893 -0.06211667 ...  0.0...   \n",
       "\n",
       "                                          review_vector  cluster  \\\n",
       "0     [ 2.31793001e-02  2.13883575e-02 -1.60253830e-...        5   \n",
       "1     [ 1.53331067e-02  9.13699530e-03 -9.21477564e-...        6   \n",
       "2     [-2.02544983e-02  2.36782003e-02 -1.38547458e-...        3   \n",
       "3     [ 1.52944177e-02  1.75527278e-02 -8.45169462e-...        8   \n",
       "4     [ 9.40260012e-04  9.34662111e-03 -1.31993005e-...        7   \n",
       "...                                                 ...      ...   \n",
       "1995  [ 1.57586411e-02  3.99073912e-03 -3.65359411e-...        0   \n",
       "1996  [-1.72636751e-02  2.09203716e-02 -1.16959680e-...        0   \n",
       "1997  [-1.72636751e-02  2.09203716e-02 -1.16959680e-...        0   \n",
       "1998  [ 1.26226731e-02  4.72505391e-02 -1.89940874e-...        0   \n",
       "1999  [-9.46927350e-03  2.49222778e-02  2.05155313e-...        0   \n",
       "\n",
       "      distance_to_center  \n",
       "0               0.244511  \n",
       "1               0.245582  \n",
       "2               0.246489  \n",
       "3               0.247793  \n",
       "4               0.249408  \n",
       "...                  ...  \n",
       "1995            0.300459  \n",
       "1996            0.300505  \n",
       "1997            0.300505  \n",
       "1998            0.300581  \n",
       "1999            0.300695  \n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_labeling  = pd.read_csv('Reviews_For_Labeling.csv')\n",
    "\n",
    "# LLM Initialization\n",
    "XAI_API_KEY = os.getenv(\"XAI_API_KEY\")\n",
    "client = OpenAI(\n",
    "    api_key=XAI_API_KEY,\n",
    "    base_url=\"https://api.x.ai/v1\",)\n",
    "\n",
    "\n",
    "# Use Batch to do Soft lableing\n",
    "def _labeling_batch(review_list):\n",
    "    batch_prompt = \"\"\n",
    "    for i, review in enumerate(review_list):\n",
    "        batch_prompt += f\"\\nReview {i+1}: {review}\"\n",
    "\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": '''\n",
    "You are a helpful assistant that summarizes user reviews related to aviation (airlines or airports). Your goal is to write a **concise and informative summary** of each post, keeping the key points and tone.\n",
    "\n",
    "Rules:\n",
    "1. Output a JSON list with the summaries for each review.\n",
    "2. Match the order exactly.\n",
    "3. Keep each summary short (1-2 sentences).\n",
    "4. Return JSON in format: {\"summaries\": [\"...\", \"...\", ...]}\n",
    "\n",
    "''' + batch_prompt\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"grok-2-latest\",\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            messages=[system_message]\n",
    "        )\n",
    "\n",
    "        response = json.loads(completion.choices[0].message.content)\n",
    "        return response[\"summaries\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch labeling: {e}\")\n",
    "        return [\"\"] * len(review_list)\n",
    "\n",
    "\n",
    "# Execute Labeling Process\n",
    "reviews = data_for_labeling.Review.tolist()\n",
    "batch_size = 20\n",
    "summaries = []\n",
    "for i in tqdm(range(0, len(reviews), batch_size), ncols=100):\n",
    "    batch = reviews[i:i + batch_size]\n",
    "    batch_summaries = _labeling_batch(batch)\n",
    "    summaries.extend(batch_summaries)\n",
    "    sleep(1)\n",
    "data_for_labeling['summary'] = summaries\n",
    "\n",
    "# Data For Finetune\n",
    "data_for_labeling.to_csv('Reviews_for_training.csv',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.2 ('DATA607_Project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1aef26605aa545d91d3fdfced60f5a4ad33a33223899fb2354864dd8410b95af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
